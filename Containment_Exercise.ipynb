{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Containment\n",
    "\n",
    "In this notebook, you'll implement a containment function that looks at a source and answer text and returns a *normalized* value that represents the similarity between those two texts based on their n-gram intersection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram counts\n",
    "\n",
    "One of the first things you'll need to do is to count up the occurrences of n-grams in your text data. To convert a set of text data into a matrix of counts, you can use a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "Below, you can set a value for n and use a CountVectorizer is used to count up the n-gram occurrences. In the next cell, we'll see that the CountVectorizer constructs a vocabulary, and later, we'll look at the matrix of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 5, 'is': 2, 'an': 0, 'answer': 1, 'text': 4, 'source': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "a_text = \"This is an answer text\"\n",
    "s_text = \"This is a source text\"\n",
    "\n",
    "# set n\n",
    "n = 1\n",
    "\n",
    "# instantiate an ngram counter\n",
    "counts = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "\n",
    "# create a dictionary of n-grams by calling `.fit`\n",
    "vocab2int = counts.fit([a_text, s_text]).vocabulary_\n",
    "\n",
    "# print dictionary of words:index\n",
    "print(vocab2int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Create a vocabulary for 2-grams (aka \"bigrams\")\n",
    "\n",
    "Create a `CountVectorizer`, `counts_2grams`, and fit it to our text data. Print out the resultant vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this is': 5, 'is an': 2, 'an answer': 0, 'answer text': 1, 'is source': 3, 'source text': 4}\n"
     ]
    }
   ],
   "source": [
    "# set n\n",
    "n = 2\n",
    "\n",
    "# instantiate an ngram counter\n",
    "counts_2 = CountVectorizer(analyzer='word', ngram_range=(n,n))\n",
    "\n",
    "# create a vocabulary for 2-grams\n",
    "counts_2grams = counts_2.fit([a_text, s_text]).vocabulary_\n",
    "\n",
    "# print dictionary of 2-grams words:index\n",
    "print(counts_2grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes up a word?\n",
    "\n",
    "You'll note that the word \"a\" does not appear in the vocabulary. And also that the words have been converted to lowercase. When `CountVectorizer` is passed `analyzer='word'` it defines a word as *two or more* characters and so it ignores uni-character words. In a lot of text analysis, single characters are often irrelevant to the meaning of a passage, so leaving them out of a vocabulary is often desired behavior. \n",
    "\n",
    "For our purposes, this default behavior will work well; we don't need uni-character words to determine cases of plagiarism, but you may still want to experiment with uni-character counts.\n",
    "\n",
    "> If you *do* want to include single characters as words, you can choose to do so by adding one more argument when creating the `CountVectorizer`; pass in the definition of a token, `token_pattern = r\"(?u)\\b\\w+\\b\"`. \n",
    "\n",
    "This regular expression defines a word as one or more characters. If you want to learn more about this vectorizer, I suggest reading through the [source code](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/feature_extraction/text.py#L664), which is well documented.\n",
    "\n",
    "**Next, let's fit our `CountVectorizer` to all of our text data to make an array of n-gram counts!**\n",
    "\n",
    "The below code, assumes that `counts` is our `CountVectorizer` for the n-gram size we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 1 1]\n",
      " [0 0 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# create array of n-gram counts for the answer and source text\n",
    "ngrams = counts.fit_transform([a_text, s_text])\n",
    "\n",
    "# row = the 2 texts and column = indexed vocab terms (as mapped above)\n",
    "# ex. column 0 = 'an', col 1 = 'answer'.. col 4 = 'text'\n",
    "ngram_array = ngrams.toarray()\n",
    "print(ngram_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the top row indicates the n-gram counts for the answer text `a_text`, and the second row indicates those for the source text `s_text`. If they have n-grams in common, you can see this by looking at the column values. For example they both have one \"is\" (column 2) and \"text\" (column 4) and \"this\" (column 5).\n",
    "\n",
    "```\n",
    "[[1 1 1 0 1 1]    =   an  answer  [is]  ______  [text] [this]\n",
    " [0 0 1 1 1 1]]   =   __  ______  [is]  source  [text] [this]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Calculate containment values\n",
    "\n",
    "Assume your function takes in an `ngram_array` just like that generated above, for an answer text (row 0) and a source text (row 1). Using just this information, calculate the containment between the two texts. As before, it's okay to ignore the uni-character words.\n",
    "\n",
    "To calculate the containment:\n",
    "1. Calculate the n-gram **intersection** between the answer and source text.\n",
    "2. Add up the number of common terms.\n",
    "3. Normalize by dividing the value in step 2 by the number of n-grams in the answer text.\n",
    "\n",
    "The complete equation is:\n",
    "\n",
    "$$ \\frac{\\sum{count(\\text{ngram}_{A}) \\cap count(\\text{ngram}_{S})}}{\\sum{count(\\text{ngram}_{A})}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def containment(ngram_array):\n",
    "    ''' Containment is a measure of text similarity. It is the normalized, \n",
    "       intersection of ngram word counts in two texts.\n",
    "       :param ngram_array: an array of ngram counts for an answer and source text.\n",
    "       :return: a normalized containment value.'''\n",
    "    \n",
    "    \n",
    "    # get the min value found in each column of the 2d array\n",
    "    intersection_list = np.amin(ngram_array, axis=0)\n",
    "\n",
    "    # sum up number of the intersection counts\n",
    "    intersection = np.sum(intersection_list)\n",
    "    \n",
    "    # count up the number of n-grams in the answer text\n",
    "    answer_idx = 0\n",
    "    answer_cnt = np.sum(ngram_array[answer_idx])\n",
    "    \n",
    "    # normalize and get final containment value\n",
    "    containment_val = intersection / answer_cnt\n",
    "\n",
    "    return containment_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containment:  0.6\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test out your code\n",
    "containment_val = containment(ngrams.toarray())\n",
    "\n",
    "print('Containment: ', containment_val)\n",
    "\n",
    "# note that for the given texts, and n = 1\n",
    "# the containment value should be 3/5 or 0.6\n",
    "assert containment_val==0.6, 'Unexpected containment value for n=1.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Containment for n=2 :  0.25\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# test for n = 2\n",
    "counts_2grams = CountVectorizer(analyzer='word', ngram_range=(2,2))\n",
    "bigram_counts = counts_2grams.fit_transform([a_text, s_text])\n",
    "\n",
    "# calculate containment\n",
    "containment_val = containment(bigram_counts.toarray())\n",
    "\n",
    "print('Containment for n=2 : ', containment_val)\n",
    "\n",
    "# the containment value should be 1/4 or 0.25\n",
    "assert containment_val==0.25, 'Unexpected containment value for n=2.'\n",
    "print('Test passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recommend trying out different phrases, and different values of n. What happens if you count for uni-character words? What if you make the sentences much larger?\n",
    "\n",
    "I find that the best way to understand a new concept is to think about how it might be applied in a variety of different ways."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
